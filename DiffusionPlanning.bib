@online{ajayConditionalGenerativeModeling2023a,
  title = {Is {{Conditional Generative Modeling}} All You Need for {{Decision-Making}}?},
  author = {Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua and Jaakkola, Tommi and Agrawal, Pulkit},
  date = {2023-07-10},
  eprint = {2211.15657},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.15657},
  url = {http://arxiv.org/abs/2211.15657},
  urldate = {2024-02-14},
  abstract = {Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/andreasburger/Zotero/storage/CS944C47/Ajay et al. - 2023 - Is Conditional Generative Modeling all you need fo.pdf}
}

@online{huangDiffusionbasedGenerationOptimization2023,
  title = {Diffusion-Based {{Generation}}, {{Optimization}}, and {{Planning}} in {{3D Scenes}}},
  author = {Huang, Siyuan and Wang, Zan and Li, Puhao and Jia, Baoxiong and Liu, Tengyu and Zhu, Yixin and Liang, Wei and Zhu, Song-Chun},
  date = {2023-01-14},
  eprint = {2301.06015},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.06015},
  url = {http://arxiv.org/abs/2301.06015},
  urldate = {2024-02-14},
  abstract = {We introduce SceneDiffuser, a conditional generative model for 3D scene understanding. SceneDiffuser provides a unified model for solving scene-conditioned generation, optimization, and planning. In contrast to prior works, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy, SceneDiffuser jointly formulates the scene-aware generation, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differentiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previous scene-conditioned generative models. We evaluate SceneDiffuser with various 3D scene understanding tasks, including human pose and motion generation, dexterous grasp generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant improvements compared with previous models, demonstrating the tremendous potential of SceneDiffuser for the broad community of 3D scene understanding.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/andreasburger/Zotero/storage/SP4HJS9C/Huang et al. - 2023 - Diffusion-based Generation, Optimization, and Plan.pdf}
}

@online{jannerPlanningDiffusionFlexible2022a,
  title = {Planning with {{Diffusion}} for {{Flexible Behavior Synthesis}}},
  author = {Janner, Michael and Du, Yilun and Tenenbaum, Joshua B. and Levine, Sergey},
  date = {2022-12-20},
  eprint = {2205.09991},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.09991},
  url = {http://arxiv.org/abs/2205.09991},
  urldate = {2024-02-14},
  abstract = {Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/andreasburger/Zotero/storage/2AXWS7R9/Janner et al. - 2022 - Planning with Diffusion for Flexible Behavior Synt.pdf}
}

@online{liangAdaptDiffuserDiffusionModels2023,
  title = {{{AdaptDiffuser}}: {{Diffusion Models}} as {{Adaptive Self-evolving Planners}}},
  shorttitle = {{{AdaptDiffuser}}},
  author = {Liang, Zhixuan and Mu, Yao and Ding, Mingyu and Ni, Fei and Tomizuka, Masayoshi and Luo, Ping},
  date = {2023-05-12},
  eprint = {2302.01877},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.01877},
  url = {http://arxiv.org/abs/2302.01877},
  urldate = {2024-02-14},
  abstract = {Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For example, AdaptDiffuser not only outperforms the previous art Diffuser by 20.8\% on Maze2D and 7.5\% on MuJoCo locomotion, but also adapts better to new tasks, e.g., KUKA pick-and-place, by 27.9\% without requiring additional expert data. More visualization results and demo videos could be found on our project page.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/andreasburger/Zotero/storage/DY6DELDR/Liang et al. - 2023 - AdaptDiffuser Diffusion Models as Adaptive Self-e.pdf}
}

@online{liangSkillDiffuserInterpretableHierarchical2023,
  title = {{{SkillDiffuser}}: {{Interpretable Hierarchical Planning}} via {{Skill Abstractions}} in {{Diffusion-Based Task Execution}}},
  shorttitle = {{{SkillDiffuser}}},
  author = {Liang, Zhixuan and Mu, Yao and Ma, Hengbo and Tomizuka, Masayoshi and Ding, Mingyu and Luo, Ping},
  date = {2023-12-18},
  eprint = {2312.11598},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.11598},
  url = {http://arxiv.org/abs/2312.11598},
  urldate = {2024-02-14},
  abstract = {Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent and long-horizon trajectories from high-level instructions remains challenging, especially for complex tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. It allows for generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior following abstract instructions across diverse tasks. Experiments on multi-task robotic manipulation benchmarks like Meta-World and LOReL demonstrate state-of-the-art performance and human-interpretable skill representations from SkillDiffuser.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/andreasburger/Zotero/storage/WY736CC6/Liang et al. - 2023 - SkillDiffuser Interpretable Hierarchical Planning.pdf}
}

@online{yangMasteringTexttoImageDiffusion2024,
  title = {Mastering {{Text-to-Image Diffusion}}: {{Recaptioning}}, {{Planning}}, and {{Generating}} with {{Multimodal LLMs}}},
  shorttitle = {Mastering {{Text-to-Image Diffusion}}},
  author = {Yang, Ling and Yu, Zhaochen and Meng, Chenlin and Xu, Minkai and Ermon, Stefano and Cui, Bin},
  date = {2024-02-05},
  eprint = {2401.11708},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.11708},
  url = {http://arxiv.org/abs/2401.11708},
  urldate = {2024-02-14},
  abstract = {Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: https://github.com/YangLing0818/RPG-DiffusionMaster},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/andreasburger/Zotero/storage/D6BJ4QXM/Yang et al. - 2024 - Mastering Text-to-Image Diffusion Recaptioning, P.pdf}
}

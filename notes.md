# Diffusion planning in the Maze2D domain

## FAQ

What is Multi2D?
- Modification of the original Maze2D, with randomized goal location at the beginning of each episode ('multi-task')

Is Maze2D discrete or continuous?
- Even though it looks discrete, it is continuous. The agent can move in any direction, but the environment is designed to look like a grid.

Are we rendering the diffuser output or is the rendering the input to the diffuser?
- The rendering is just for visualization. The input and output to the diffuser is (batch_size, planning_horizon, state_dim + action_dim)

How are the start and endpoints given to the diffuser?
- inpainting strategy to condition on a start and goal location
- generalize by simply change the conditioning goal 
- condition the trajectories on the current state using the inpainting procedure
- no conditioning in the sense of classifier / classifier-free guidance?

How do we ensure the walls are not crossed?
- We don't enforce that collision constraint explicitly; the samples respect this by virtue of there being no data that walks through the walls [see github issue](https://github.com/jannerm/diffuser/issues/6)

What data is the diffuser trained on?
- training data in Maze2D is undirected â€“ consisting of a controller navigating to and from randomly selected locations
- You can see the training data in `logs/<task>/diffusion/<>/_sample-reference.png` 
(generated by `diffuser/utils/training.py Trainer.render_reference`)

How many steps do we plan at once?
- planning horizon T of 128 in U-Maze, 265 in Medium, and 384 in Large (Maze2D / Multi2D)
- we could replan every few steps (open loop planning), but the authors say planning once works just fine for Maze2D in practice

How large are the default (small) mazes?
- from `diffuser/utils/rendering.py`: 
MAZE_BOUNDS = {
    "maze2d-umaze-v1": (0, 5, 0, 5),
    "maze2d-medium-v1": (0, 8, 0, 8),
    "maze2d-large-v1": (0, 9, 0, 12),
}
- (0,0) is the top left corner
- (0,12) is the top right corner

What happens if we train a diffuser on a larger maze?
- We can't train a diffuser on a larger maze, because we don't have any training data for larger mazes. The diffuser can only imitate another agent who can solve the larger maze.

Can diffuser output trajectories with 'holes'?
- No. The diffuser controls a ball. The output are linear forces on the ball (x_force, y_force). The observation space is (x_coord, y_coord, x_velocity, y_velocity).

How long does the diffusion model take to train? 
- A separate model is trained for each maze size. On one RTX3060, with default parameters:
- maze2d-large-v1: 20h (200 epochs, ~1000 training trajectories per epoch)
- maze2d_umaze_v1:
- maze2d_medium_v1:

How many parameters does the diffusion model have?
- UNet with 3.7M parameters
- same architecture for all maze sizes, but trained separately



## Our approach

- Keep the diffusion model (e.g. trained on maze2d-large-v1) as is, without any additional training
- Construct a larger maze by concatenating multiple smaller mazes
- Discretize the larger maze into a grid for the planner
- Get the planner to output a trajectory in the larger maze
- Sample waypoints on the planner trajectory and use them as start and goal locations for the diffuser
- Diffuser only acts on mazes the size it was trained on 

- size of maze2d-large-v1 = (S x S)
- concatenate L^2 mazes to get a maze of size (L*S x L*S)

Limitations
- diffuser can only be trained on one maze layout, because walls are not enforced as constraints. The diffuser does not get the walls as information, it only sees that the agent's only move in in some parts of the maze. -> add the walls as constraints to the diffuser
- diffuser gets reward if it reaches the goal within 0.5 units of the goal, so it never quite reaches the goal. -> change the reward function to be more strict
